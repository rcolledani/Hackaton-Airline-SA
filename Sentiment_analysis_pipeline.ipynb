{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvZRVbLY1wdT"
   },
   "source": [
    "# General overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD222R4p1wdV"
   },
   "source": [
    "This notebook presents our work in analyzing the feelings of aircraft equipment. \n",
    "The objective of this project is to find out what people think of this equipment in order to improve customer service on the aircraft concerned.\n",
    "To achieve this, we have divided our work into two main parts: \n",
    "- A part of pre-processing our data and obtaining what is important to us; to do this, we examine a significant sample of 3,000 comments in order to obtain the main subjects of each comment. \n",
    "- A sentiment analysis model; based on these topics, we look at all the comments to get people's opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhaBMwFf1wdV"
   },
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IyWY5xe2gsf"
   },
   "outputs": [],
   "source": [
    "!pip install aspect_based_sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HTYYka0-1wdW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import aspect_based_sentiment_analysis as absa\n",
    "from collections import Counter\n",
    "from collections import Iterable\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Smx-R-H1wdb"
   },
   "outputs": [],
   "source": [
    "#  To improve computation speed - run on GPU\n",
    "get_ipython().run_line_magic('tensorflow_version', '2.x')\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hkaemmy51wdW"
   },
   "outputs": [],
   "source": [
    "#  Load up our dataset\n",
    "col_names = [\n",
    "    \"review_ID\",\n",
    "    \"date_published\",\n",
    "    \"global_ratings\",\n",
    "    \"reviews_titles\",\n",
    "    \"customers_countries\",\n",
    "    \"reviews_body\",\n",
    "    \"is_verified\",\n",
    "    \"aircraft\",\n",
    "    \"type_traveller\",\n",
    "    \"seat_type\",\n",
    "    \"route_provenance\",\n",
    "    \"route_destination\",\n",
    "    \"date_flown\",\n",
    "    \"seat_comfort\",\n",
    "    \"food_beverages\",\n",
    "    \"cabin_staff_service\",\n",
    "    \"sleep_comfort\",\n",
    "    \"sitting_comfort\",\n",
    "    \"seat_width\",\n",
    "    \"seat_length\",\n",
    "    \"seat_privac\",\n",
    "    \"power_supply\",\n",
    "    \"seat_storage\",\n",
    "    \"is_recommended\",\n",
    "    \"is_airline_review\",\n",
    "    \"airline_name\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"airlines_dataset_exhaustive.csv\",\n",
    "    names=col_names,\n",
    "    skiprows=0,\n",
    "    delimiter=\";\",\n",
    "    low_memory=False,\n",
    "    encoding='utf-8')\n",
    "df = df.iloc[1:]\n",
    "\n",
    "#  Print our data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvaAK52G1wdW"
   },
   "source": [
    "# Preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i-hyC07J1wdX"
   },
   "outputs": [],
   "source": [
    "#  Load NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nCmUNBd1wdX"
   },
   "source": [
    "## 1) Get the main aspects based on the column 'is_airline_review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VNUlAms_1wdX"
   },
   "outputs": [],
   "source": [
    "#  Create a function to flat our list\n",
    "def flatten(lis):\n",
    "    for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "            for x in flatten(item):\n",
    "                 yield x\n",
    "        else:        \n",
    "             yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U6MI1yWi1wdX"
   },
   "outputs": [],
   "source": [
    "#  Lemmatize a column in pandas\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZU5pBQl1wdY"
   },
   "outputs": [],
   "source": [
    "#  Select only reviews with is_airline_review = 0\n",
    "with tf.device('/device:GPU:0'):\n",
    "    df_reduce = df[df['is_airline_review'] == '0']\n",
    "\n",
    "    # Create a column 'aspect terms' containing the aspects of each comments\n",
    "    # (based on the nouns used in a comment)\n",
    "    df_reduce.reviews_body = df_reduce.reviews_body.str.lower()\n",
    "\n",
    "    aspect_terms = []\n",
    "    for review in nlp.pipe(df_reduce.reviews_body):\n",
    "        chunks = [(chunk.root.text)\n",
    "                  for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "        aspect_terms.append(' '.join(chunks))\n",
    "    df_reduce['aspect_terms'] = aspect_terms\n",
    "    df_reduce.head(10)\n",
    "\n",
    "    #  Lemmatizate the 'aspect_terms' columns to clean the words\n",
    "    nltk.download('wordnet')\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    df_reduce['aspect_terms'] = df_reduce.aspect_terms.apply(lemmatize_text)\n",
    "\n",
    "    # Convert to list\n",
    "    different_aspects = df_reduce['aspect_terms'].tolist()\n",
    "    different_aspects = list(flatten(different_aspects))\n",
    "    split_on_these = [' ']\n",
    "    allwords = []\n",
    "\n",
    "    # Convert comments to unique words\n",
    "    for i in range(len(different_aspects)):\n",
    "        words = different_aspects[i].split()\n",
    "        allwords.append(words)\n",
    "\n",
    "    #  Flat our list and get the most frequent words\n",
    "    allwords = list(flatten(allwords))\n",
    "    x = Counter(allwords)\n",
    "    most_common = x.most_common()\n",
    "\n",
    "    #  Get the 500 words which occur the most\n",
    "    top_500 = most_common[0:500]\n",
    "\n",
    "    # Get the most common elements on list format\n",
    "    first_tuple_elements = []\n",
    "\n",
    "    for a_tuple in top_500:\n",
    "        first_tuple_elements.append(a_tuple[0])\n",
    "\n",
    "# Emport the top 500 words into .csv\n",
    "export = pd.DataFrame(first_tuple_elements)\n",
    "export.to_csv(r\"file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKhgBIT01wdY"
   },
   "source": [
    "We exported the first 500 words to an Excel spreadsheet. Then we manually selected 40 words that were significant for our aspect based analysis. This selection was based on common sense and no NLP algorithm could have helped us accomplish this task;\n",
    "Many words appeared in different handwritings, many synonyms were present, some words had no meaning, and other words were not related to the equipment... which was useless considering our client.\n",
    "We then did our sentiment analysis based on these 40 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uL5G6XzN1wdY"
   },
   "outputs": [],
   "source": [
    "#  Import our 40 chosen words\n",
    "words_chosen = pd.read_csv(\"words.csv\", header=None)\n",
    "\n",
    "#  Convert to list\n",
    "words_chosen = words_chosen[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wirw-VXJ1wdZ"
   },
   "source": [
    "## 2) Get aspect terms on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH7D9lqb1wdZ"
   },
   "source": [
    "Now that we have our chosen words (our topics), we can retrieve on the whole dataset the sentiment of people on these words. \n",
    "On this part, we kept only reviews containing at least one aspect present in our chosen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY_XhcYy1wdZ"
   },
   "outputs": [],
   "source": [
    "# Get the aspect terms of each comments based on the nouns : ATTENTION\n",
    "# about 1h30 running on CPU\n",
    "with tf.device('/device:GPU:0'):\n",
    "    df.reviews_body = df.reviews_body.str.lower()\n",
    "\n",
    "    aspect_terms = []\n",
    "    for review in nlp.pipe(df.reviews_body):\n",
    "        chunks = [(chunk.root.text)\n",
    "                  for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
    "        aspect_terms.append(' '.join(chunks))\n",
    "    df['aspect_terms'] = aspect_terms\n",
    "\n",
    "    #  Lemmatize 'aspect_terms' on the whole dataset\n",
    "    df['aspect_terms_lem'] = df.aspect_terms.apply(lemmatize_text)\n",
    "\n",
    "    #  Create a new column containing only words of interest\n",
    "    df = df.assign(relevant_aspects=[\n",
    "                  [x for x in y if x in words_chosen] for y in df.aspect_terms_lem])\n",
    "\n",
    "    #  Get only unique values\n",
    "    for i in range(1, len(df)):\n",
    "        df['relevant_aspects'][i] = list(set(df['relevant_aspects'][i]))\n",
    "\n",
    "    #  Convert list to string\n",
    "    df['relevant_aspects'] = df.relevant_aspects.apply(\n",
    "        lambda x: ', '.join([str(i) for i in x]))\n",
    "\n",
    "    #  Keep only rows with aspects in words chosen\n",
    "    airlines_comment = df[df['relevant_aspects'] != '']\n",
    "\n",
    "    #  Lemmatizate comments\n",
    "    airlines_comment['reviews_body_lem'] = airlines_comment.reviews_body.apply(\n",
    "        lemmatize_text)\n",
    "\n",
    "    #  Convert list to string\n",
    "    airlines_comment['reviews_body_lem'] = airlines_comment.reviews_body_lem.apply(\n",
    "        lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "    #  Reorganised columns\n",
    "    airlines_comment['reviews_body'] = airlines_comment['reviews_body_lem']\n",
    "    airlines_comment = airlines_comment.drop(\n",
    "        ['reviews_body_lem', 'aspect_terms_lem'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdcuDXtu1wda"
   },
   "source": [
    "# Model - sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXVsGB3B1wda"
   },
   "source": [
    "On this part, we got the opinion of people regarding the chosen words based on their comments. \n",
    "The sentiment analysis model come from the 'aspect_based_sentiment_analysis' library, which is the very well-known BERT model for its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IR3LiXME1wda"
   },
   "outputs": [],
   "source": [
    "def get_relevant_qualifiers(aspect, k):\n",
    "    weights = np.array(aspect.review.patterns[0].weights)\n",
    "    idx = np.argpartition(weights, -k)[-k:]\n",
    "    text = np.array(aspect.text_tokens)\n",
    "    return text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yvgtqyv21wdb"
   },
   "outputs": [],
   "source": [
    "#  Load aspect based sentiment analysis \n",
    "nlp = absa.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EoLvNo_Y1wdc"
   },
   "outputs": [],
   "source": [
    "#Apply our model - 4h running on CPU\n",
    "\n",
    "#  Get only columns needed for the analysis\n",
    "comments = airlines_comment['reviews_body']\n",
    "aspects = airlines_comment['relevant_aspects']\n",
    "ids = airlines_comment['review_ID']\n",
    "\n",
    "#  Convert to list to avoir error iteration\n",
    "ids = list(ids)\n",
    "comments = list(comments)\n",
    "aspects = list(aspects)\n",
    "\n",
    "#  Apply sentiment analysis on our reviews\n",
    "with tf.device('/device:GPU:0'):\n",
    "    recognizer = absa.aux_models.BasicPatternRecognizer()\n",
    "    nlp = absa.load(pattern_recognizer=recognizer)\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(airlines_comment)):\n",
    "        comment = comments[i][:512]  # Limit commments to 512 caracters\n",
    "        aspect = list(str(aspects[i]).split(\", \"))\n",
    "        # Get only 4 aspects max by review (computation)\n",
    "        n = min(4, len(aspect))\n",
    "        completed_task = nlp(text=str(comment), aspects=aspect[0:n])\n",
    "        aspect1 = completed_task.examples\n",
    "        result_per_aspect = []\n",
    "\n",
    "        for j in range(n):\n",
    "            score = max(aspect1[j].scores)\n",
    "            tendance = aspect1[j].scores.index(score)\n",
    "            words = get_relevant_qualifiers(aspect1[j], 3)\n",
    "            result_per_aspect.append((aspect[j], tendance, score, words))\n",
    "\n",
    "        result.append((i, result_per_aspect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCgAj4Jy1wdc"
   },
   "outputs": [],
   "source": [
    "#  Get the final result in data frame\n",
    "rev = []\n",
    "asp = []\n",
    "tendance = []\n",
    "scor = []\n",
    "context1 = []\n",
    "context2 = []\n",
    "context3 = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    nb_aspect = len(result[i][1])\n",
    "    for j in range(nb_aspect):\n",
    "        # We add nb_aspect time the review_id on the review_id column\n",
    "        rev.append(result[i][0])\n",
    "        asp.append(result[i][1][j][0])\n",
    "\n",
    "    #  Create column to put negativity, neutrality and postivity scores\n",
    "    trend = result[i][1][j][1]\n",
    "    if (trend == 0):\n",
    "        tendance.append(\"NEGATIVE\")\n",
    "    elif (trend == 1):\n",
    "        tendance.append(\"NEUTRAL\")\n",
    "    elif (trend == 2):\n",
    "        tendance.append(\"POSITIVE\")\n",
    "\n",
    "    # Get the score for each type of sentiment (neutral, negative and\n",
    "    # positive)\n",
    "    scor.append(result[i][1][j][2])\n",
    "    context1.append(result[i][1][j][3][0])\n",
    "    context2.append(result[i][1][j][3][1])\n",
    "    context3.append(result[i][1][j][3][2])\n",
    "\n",
    "#  Set data frame\n",
    "d = {\n",
    "    'review_id': rev,\n",
    "    'aspect': asp,\n",
    "    'tendance': tendance,\n",
    "    'score': scor,\n",
    "    'context_1': context1,\n",
    "    'context_2': context2,\n",
    "    'context_3': context3}\n",
    "final_df = pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmguXnEq1wdc"
   },
   "outputs": [],
   "source": [
    "#  Print the final result\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Sentiment_analysis_pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
